{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yifeiz-bu/Industrial-project/blob/main/YMT_product_label_classification_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCPm3Rz3ch9T"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwqYcCuqHKf3",
        "outputId": "71b3a164-9666-4085-db1e-e8d4264551a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
            "Collecting python-Levenshtein\n",
            "  Downloading python_Levenshtein-0.26.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting Levenshtein==0.26.0 (from python-Levenshtein)\n",
            "  Downloading levenshtein-0.26.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.26.0->python-Levenshtein)\n",
            "  Downloading rapidfuzz-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Downloading python_Levenshtein-0.26.0-py3-none-any.whl (9.4 kB)\n",
            "Downloading levenshtein-0.26.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
            "Successfully installed Levenshtein-0.26.0 python-Levenshtein-0.26.0 rapidfuzz-3.10.0\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (0.42.1)\n",
            "Collecting wget==3.2\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=3852b3294cd3303658d804444e1b5c47169eed9934845ff13f36a1ec0ad97888\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.8.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "#Make the necessary imports\n",
        "import os\n",
        "import sys\n",
        "import tarfile\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from zipfile import ZipFile\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.initializers import Constant\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "!pip install gensim\n",
        "!pip install python-Levenshtein\n",
        "import multiprocessing\n",
        "from gensim.models import Word2Vec\n",
        "!pip install jieba\n",
        "import jieba\n",
        "import string\n",
        "!pip install wget==3.2\n",
        "!pip install tensorflow\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset,TensorDataset\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEVj1QHFuqzW",
        "outputId": "634684ea-80bc-4b8f-8311-d00401e690fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMuPlQmHqjQ0"
      },
      "outputs": [],
      "source": [
        "# Path to your model file on Google Drive\n",
        "pretrained_model = '/content/drive/MyDrive/model_file'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5330dRs839Iy"
      },
      "outputs": [],
      "source": [
        "#Load pre trained model\n",
        "w2v_model = '/content/sgns.weibo.bigram-char'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmLOMt7lHub2"
      },
      "outputs": [],
      "source": [
        "w2v_model2 = '/content/drive/My Drive/sgns.merge.bigram'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNgFSdFixhIE"
      },
      "source": [
        "# data feature engineering and cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU3e0_a1tEaz"
      },
      "source": [
        "load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZU5GTgAvTY7"
      },
      "outputs": [],
      "source": [
        "df1 = pd.read_csv('/content/drive/My Drive/8031.csv',sep='\\t',names=['IN code','pk','description'])\n",
        "df2 = pd.read_csv('/content/drive/My Drive/8453.csv',sep='\\t',names=['IN code','pk','description'])\n",
        "df3 = pd.read_csv('/content/drive/My Drive/8199.csv',sep='\\t',names=['IN code','pk','description'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zI7aJ7XOEBu3"
      },
      "outputs": [],
      "source": [
        "del df1['pk'],df2['pk'],df3['pk']\n",
        "#concatation\n",
        "df = pd.concat([df1, df2,df3],ignore_index = True,sort = False)\n",
        "df_ori = df[(df['IN code']== 8031) | (df['IN code']== 8453) |(df['IN code']== 8199)].reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGqTE0-R6fs2",
        "outputId": "573fda1c-5bb2-4c9a-c7d8-20e3cde35a2b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>精品黄金梨又甜又脆，口感非常的好，货好的很</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>酥梨 600g以上 膜袋</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>河北皇冠梨赵县梨新鲜梨子 品质好 价格优 欢迎来聊</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>万亩梨园有机富硒梨长十郎 皇冠梨，黄金梨</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>苏翠一号梨大量上市口感好糖度高</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13963</th>\n",
              "      <td>冰糖心苹果红富士苹果有机苹果水果批发脆甜可口绿色食品</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13964</th>\n",
              "      <td>红星苹果 80mm以上 膜袋  甜 糖心 红 批发</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13965</th>\n",
              "      <td>铜川嘎啦苹果，口感甜脆，健康又美味。</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13966</th>\n",
              "      <td>山东烟台栖霞红富士苹果即将大量上市</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13967</th>\n",
              "      <td>烟台一二级80苹果</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13968 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ],
            "text/plain": [
              "0             精品黄金梨又甜又脆，口感非常的好，货好的很\n",
              "1                      酥梨 600g以上 膜袋\n",
              "2         河北皇冠梨赵县梨新鲜梨子 品质好 价格优 欢迎来聊\n",
              "3              万亩梨园有机富硒梨长十郎 皇冠梨，黄金梨\n",
              "4                   苏翠一号梨大量上市口感好糖度高\n",
              "                    ...            \n",
              "13963    冰糖心苹果红富士苹果有机苹果水果批发脆甜可口绿色食品\n",
              "13964     红星苹果 80mm以上 膜袋  甜 糖心 红 批发\n",
              "13965            铜川嘎啦苹果，口感甜脆，健康又美味。\n",
              "13966             山东烟台栖霞红富士苹果即将大量上市\n",
              "13967                     烟台一二级80苹果\n",
              "Name: description, Length: 13968, dtype: object"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_ori['description'] #inspect the original dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0cUYPM4IcrQ"
      },
      "outputs": [],
      "source": [
        "df = df_ori.copy() #duplicate the experimental dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DEsDordpx4u"
      },
      "source": [
        "Remove punctuation and white space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quGmSBg2sV_4",
        "outputId": "d0269f5a-76f0-4c76-d0b2-7f47ee923ef9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~。，！？、；（）【】《》…—‘’“”‘'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "string.punctuation + '。'+'，'+'！'+'？'+'、'+'；' +'（'+'）'+'【'+'】'+'《'+'》'+'…'+'—'+'‘'+'’'+'“'+'”'+'‘'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuFbdTPts6-S",
        "outputId": "8d2d200e-ce6e-450f-aa2e-ab78ed72e347"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~。，！？、；（）【】《》…—‘’“”‘'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "punct_combined = string.punctuation + '。'+'，'+'！'+'？'+'、'+'；' +'（'+'）'+'【'+'】'+'《'+'》'+'…'+'—'+'‘'+'’'+'“'+'”'+'‘'\n",
        "punct_combined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5ysG6oPSd7R",
        "outputId": "56827910-59f3-4872-b989-c50ade23e99b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 13968,\n  \"fields\": [\n    {\n      \"column\": \"index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4376,\n        \"min\": 0,\n        \"max\": 14999,\n        \"num_unique_values\": 13968,\n        \"samples\": [\n          7327,\n          5857,\n          10098\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"IN code\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 171,\n        \"min\": 8031,\n        \"max\": 8453,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          8031,\n          8199,\n          8453\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 13851,\n        \"samples\": [\n          \"\\u8001\\u82b1\\u725b\\u82f9\\u679c\",\n          \"\\u81ea\\u7136\\u6210\\u719f\\u9999\\u5473\\u6d53\\u90c1\\u91cf\\u5927\\u8d28\\u4f18\\u5927\\u9999\\u8549\",\n          \"\\u5c71\\u897f\\u4e34\\u6c7e\\u8106\\u751c\\u51b0\\u7cd6\\u5fc3\\u82f9\\u679c\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-aea52514-e8cc-4a85-9db8-fb162b5dc9da\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>IN code</th>\n",
              "      <th>description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>8031</td>\n",
              "      <td>精品黄金梨又甜又脆口感非常的好货好的很</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>8031</td>\n",
              "      <td>酥梨 g以上 膜袋</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>8031</td>\n",
              "      <td>河北皇冠梨赵县梨新鲜梨子 品质好 价格优 欢迎来聊</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>8031</td>\n",
              "      <td>万亩梨园有机富硒梨长十郎 皇冠梨黄金梨</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>8031</td>\n",
              "      <td>苏翠一号梨大量上市口感好糖度高</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aea52514-e8cc-4a85-9db8-fb162b5dc9da')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-aea52514-e8cc-4a85-9db8-fb162b5dc9da button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-aea52514-e8cc-4a85-9db8-fb162b5dc9da');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-799d23f1-78fb-45c4-b7b9-123b70e0ec30\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-799d23f1-78fb-45c4-b7b9-123b70e0ec30')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-799d23f1-78fb-45c4-b7b9-123b70e0ec30 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   index  IN code                description\n",
              "0      0     8031        精品黄金梨又甜又脆口感非常的好货好的很\n",
              "1      1     8031                  酥梨 g以上 膜袋\n",
              "2      3     8031  河北皇冠梨赵县梨新鲜梨子 品质好 价格优 欢迎来聊\n",
              "3      4     8031        万亩梨园有机富硒梨长十郎 皇冠梨黄金梨\n",
              "4      5     8031            苏翠一号梨大量上市口感好糖度高"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# a list to store non-punctuation words\n",
        "non_punct = []\n",
        "\n",
        "# loops through each sentence in the `title` column and then checks each word in the text\n",
        "# keeps it in the letters list if non-punctuation\n",
        "for word in df['description']:\n",
        "    letters = [letter for letter in word if letter not in punct_combined and not letter.isdigit()]\n",
        "    non_punct.append(''.join(letters))\n",
        "\n",
        "df['description'] = pd.Series(non_punct)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQEZ7ob5pS71"
      },
      "source": [
        "Remove stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2Z7Sfb1pSfL"
      },
      "outputs": [],
      "source": [
        "#nltk.download('stopwords')\n",
        "#stopword = nltk.corpus.stopwords.words('chinese')\n",
        "## pring the first 11 stop words in the list\n",
        "#print(stopword[:11])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8XoQ-TbpaOW"
      },
      "outputs": [],
      "source": [
        "## a list to store stopwords\n",
        "#stopwords = []\n",
        "#\n",
        "## loops through each sentence in the `title_non_punct_split` column and then\n",
        "## stores words in a list of non_stop if the word is not a stopword\n",
        "## then finally appends each list into stopwords\n",
        "#for zi in df['description']:\n",
        "#    non_stop = [word for word in zi if word not in stopword]\n",
        "#    stopwords.append(non_stop)\n",
        "#\n",
        "#df['description'] = pd.Series(stopwords)\n",
        "#df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9_Nm2Rk1w-d"
      },
      "source": [
        "#Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fj3-qnrIqTaY",
        "outputId": "a67a3266-14e2-4e29-cc4c-b3b0515f3b74"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.596 seconds.\n",
            "DEBUG:jieba:Loading model cost 0.596 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "DEBUG:jieba:Prefix dict has been built successfully.\n"
          ]
        }
      ],
      "source": [
        "token_lis = []\n",
        "for i in df['description']:\n",
        "  # Join the list of words back into a string\n",
        "  text = ''.join(i)\n",
        "  seg_list = list(jieba.lcut(text)) # 搜索引擎模式\n",
        "  x = (','.join(seg_list))\n",
        "  token_lis.append(seg_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BscAEHOHXLAC",
        "outputId": "dde6ea12-88df-4497-c4a7-2bf812da3d4b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 13968,\n  \"fields\": [\n    {\n      \"column\": \"index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4376,\n        \"min\": 0,\n        \"max\": 14999,\n        \"num_unique_values\": 13968,\n        \"samples\": [\n          7327,\n          5857,\n          10098\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"IN code\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 171,\n        \"min\": 8031,\n        \"max\": 8453,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          8031,\n          8199,\n          8453\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 13851,\n        \"samples\": [\n          \"\\u8001\\u82b1\\u725b\\u82f9\\u679c\",\n          \"\\u81ea\\u7136\\u6210\\u719f\\u9999\\u5473\\u6d53\\u90c1\\u91cf\\u5927\\u8d28\\u4f18\\u5927\\u9999\\u8549\",\n          \"\\u5c71\\u897f\\u4e34\\u6c7e\\u8106\\u751c\\u51b0\\u7cd6\\u5fc3\\u82f9\\u679c\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"description_token\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-8e14e86c-e5fc-4746-9764-c0fe3b08425d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>IN code</th>\n",
              "      <th>description</th>\n",
              "      <th>description_token</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>8031</td>\n",
              "      <td>精品黄金梨又甜又脆口感非常的好货好的很</td>\n",
              "      <td>[精品, 黄金梨, 又, 甜, 又, 脆, 口感, 非常, 的, 好, 货好, 的, 很]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>8031</td>\n",
              "      <td>酥梨 g以上 膜袋</td>\n",
              "      <td>[酥梨,  , g, 以上,  , 膜袋]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>8031</td>\n",
              "      <td>河北皇冠梨赵县梨新鲜梨子 品质好 价格优 欢迎来聊</td>\n",
              "      <td>[河北, 皇冠, 梨, 赵县, 梨, 新鲜, 梨子,  , 品质, 好,  , 价格, 优,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>8031</td>\n",
              "      <td>万亩梨园有机富硒梨长十郎 皇冠梨黄金梨</td>\n",
              "      <td>[万亩, 梨园, 有机, 富, 硒, 梨, 长十郎,  , 皇冠, 梨, 黄金梨]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>8031</td>\n",
              "      <td>苏翠一号梨大量上市口感好糖度高</td>\n",
              "      <td>[苏翠, 一号, 梨, 大量, 上市, 口感, 好, 糖度, 高]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13963</th>\n",
              "      <td>14995</td>\n",
              "      <td>8199</td>\n",
              "      <td>冰糖心苹果红富士苹果有机苹果水果批发脆甜可口绿色食品</td>\n",
              "      <td>[冰糖, 心, 苹果, 红富士, 苹果, 有机, 苹果, 水果, 批发, 脆甜, 可口, 绿...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13964</th>\n",
              "      <td>14996</td>\n",
              "      <td>8199</td>\n",
              "      <td>红星苹果 mm以上 膜袋  甜 糖心 红 批发</td>\n",
              "      <td>[红星, 苹果,  , mm, 以上,  , 膜袋,  ,  , 甜,  , 糖心,  , ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13965</th>\n",
              "      <td>14997</td>\n",
              "      <td>8199</td>\n",
              "      <td>铜川嘎啦苹果口感甜脆健康又美味</td>\n",
              "      <td>[铜川, 嘎啦, 苹果, 口感, 甜脆, 健康, 又, 美味]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13966</th>\n",
              "      <td>14998</td>\n",
              "      <td>8199</td>\n",
              "      <td>山东烟台栖霞红富士苹果即将大量上市</td>\n",
              "      <td>[山东, 烟台, 栖霞, 红富士, 苹果, 即将, 大量, 上市]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13967</th>\n",
              "      <td>14999</td>\n",
              "      <td>8199</td>\n",
              "      <td>烟台一二级苹果</td>\n",
              "      <td>[烟台, 一, 二级, 苹果]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13968 rows × 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8e14e86c-e5fc-4746-9764-c0fe3b08425d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8e14e86c-e5fc-4746-9764-c0fe3b08425d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8e14e86c-e5fc-4746-9764-c0fe3b08425d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5fef28f1-b777-4448-a9fd-861ade87958c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5fef28f1-b777-4448-a9fd-861ade87958c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5fef28f1-b777-4448-a9fd-861ade87958c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_3d2da270-1dc4-4822-b401-c7f093bf1c1a\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_3d2da270-1dc4-4822-b401-c7f093bf1c1a button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "       index  IN code                 description  \\\n",
              "0          0     8031         精品黄金梨又甜又脆口感非常的好货好的很   \n",
              "1          1     8031                   酥梨 g以上 膜袋   \n",
              "2          3     8031   河北皇冠梨赵县梨新鲜梨子 品质好 价格优 欢迎来聊   \n",
              "3          4     8031         万亩梨园有机富硒梨长十郎 皇冠梨黄金梨   \n",
              "4          5     8031             苏翠一号梨大量上市口感好糖度高   \n",
              "...      ...      ...                         ...   \n",
              "13963  14995     8199  冰糖心苹果红富士苹果有机苹果水果批发脆甜可口绿色食品   \n",
              "13964  14996     8199     红星苹果 mm以上 膜袋  甜 糖心 红 批发   \n",
              "13965  14997     8199             铜川嘎啦苹果口感甜脆健康又美味   \n",
              "13966  14998     8199           山东烟台栖霞红富士苹果即将大量上市   \n",
              "13967  14999     8199                     烟台一二级苹果   \n",
              "\n",
              "                                       description_token  \n",
              "0          [精品, 黄金梨, 又, 甜, 又, 脆, 口感, 非常, 的, 好, 货好, 的, 很]  \n",
              "1                                  [酥梨,  , g, 以上,  , 膜袋]  \n",
              "2      [河北, 皇冠, 梨, 赵县, 梨, 新鲜, 梨子,  , 品质, 好,  , 价格, 优,...  \n",
              "3              [万亩, 梨园, 有机, 富, 硒, 梨, 长十郎,  , 皇冠, 梨, 黄金梨]  \n",
              "4                      [苏翠, 一号, 梨, 大量, 上市, 口感, 好, 糖度, 高]  \n",
              "...                                                  ...  \n",
              "13963  [冰糖, 心, 苹果, 红富士, 苹果, 有机, 苹果, 水果, 批发, 脆甜, 可口, 绿...  \n",
              "13964  [红星, 苹果,  , mm, 以上,  , 膜袋,  ,  , 甜,  , 糖心,  , ...  \n",
              "13965                    [铜川, 嘎啦, 苹果, 口感, 甜脆, 健康, 又, 美味]  \n",
              "13966                  [山东, 烟台, 栖霞, 红富士, 苹果, 即将, 大量, 上市]  \n",
              "13967                                    [烟台, 一, 二级, 苹果]  \n",
              "\n",
              "[13968 rows x 4 columns]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['description_token']=token_lis\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1WTf9eVzPj7"
      },
      "source": [
        "Create test,training sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrelY3ZgMWlg"
      },
      "outputs": [],
      "source": [
        "df['IN code'] = df_ori['IN code']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EioHdbpOzRTn",
        "outputId": "cdfc3f36-651e-4f93-d26a-b10c5167f8da"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>IN code</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8031</th>\n",
              "      <td>4785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8199</th>\n",
              "      <td>4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8453</th>\n",
              "      <td>4421</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ],
            "text/plain": [
              "IN code\n",
              "8031    4785\n",
              "8199    4762\n",
              "8453    4421\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['IN code'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLBvEV3uzbr7"
      },
      "outputs": [],
      "source": [
        "# Encoding the label column\n",
        "y = df['IN code'].astype(str).map({'8031': 1, '8453': 2,'8199':3})\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(df['description_token'], y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wce2C7mb-YOw",
        "outputId": "fb0ee761-beae-4b0b-bfd4-03781b61c51e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>IN code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4112</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4850</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12997</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3016</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12656</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5191</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13418</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5390</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>860</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7270</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11174 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ],
            "text/plain": [
              "4112     1\n",
              "4850     2\n",
              "12997    3\n",
              "3016     1\n",
              "12656    3\n",
              "        ..\n",
              "5191     2\n",
              "13418    3\n",
              "5390     2\n",
              "860      1\n",
              "7270     2\n",
              "Name: IN code, Length: 11174, dtype: int64"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaCyr27FoEly"
      },
      "source": [
        "### Apply Gensim Word2Vec model to train the word vectors based on the training data as the corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gIGZ-htSKp8"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "import warnings\n",
        "from collections import defaultdict  # Import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMKRZal7ToB0",
        "outputId": "15e23c42-23fc-48ad-d2f7-a815e027d086"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[' ', '苹果', '香蕉', '梨', '红富士', '大量', '的', '产地', '上市', '酥梨']"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences = df['description_token']  # No need to call .split() since the data is already tokenized\n",
        "word_freq = defaultdict(int)\n",
        "\n",
        "# Loop through each token in each sentence (list of tokens)\n",
        "for sent in sentences:\n",
        "    for i in sent:\n",
        "        word_freq[i] += 1\n",
        "\n",
        "# Sort and retrieve the 10 most frequent words\n",
        "top_10_words = sorted(word_freq, key=word_freq.get, reverse=True)[:10]\n",
        "top_10_words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FeK8YHP6SFPi"
      },
      "outputs": [],
      "source": [
        "# Create CBOW model\n",
        "warnings.filterwarnings('ignore')# define training data\n",
        "w2v_model = Word2Vec(df['description_token'], min_count=1,\n",
        "                                vector_size=300, window=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6onClXv09de",
        "outputId": "9ec3272d-08e6-4604-ec28-6b3ee0650021"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[' ',\n",
              " '苹果',\n",
              " '香蕉',\n",
              " '梨',\n",
              " '红富士',\n",
              " '大量',\n",
              " '的',\n",
              " '产地',\n",
              " '上市',\n",
              " '酥梨',\n",
              " '蕉',\n",
              " '精品',\n",
              " '口感',\n",
              " '好',\n",
              " '欢迎',\n",
              " '大',\n",
              " '云南',\n",
              " '有',\n",
              " '一件',\n",
              " '代发',\n",
              " '皇冠',\n",
              " '批发',\n",
              " '供应',\n",
              " '红',\n",
              " '广西',\n",
              " '水果',\n",
              " '联系',\n",
              " '新鲜',\n",
              " '了',\n",
              " '陕西',\n",
              " '直销',\n",
              " '富士',\n",
              " '货',\n",
              " '纸袋',\n",
              " '以上',\n",
              " '需要',\n",
              " '甜',\n",
              " '脆甜',\n",
              " '货源',\n",
              " '基地',\n",
              " '价格',\n",
              " '冰糖',\n",
              " '斤',\n",
              " '山东',\n",
              " '进口',\n",
              " '优质',\n",
              " 'g',\n",
              " '老板',\n",
              " '不',\n",
              " '小米']"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Represent first 50 of the words that our Word2Vec model learned a vector for.\n",
        "w2v_model.wv.index_to_key[:50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVktCDF28BF-",
        "outputId": "fc834474-f939-4c21-968b-61b0aac2f53b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('梨', 0.9954535961151123),\n",
              " ('丰水', 0.9953941106796265),\n",
              " ('雪花', 0.9953888058662415),\n",
              " ('水晶', 0.9928445816040039),\n",
              " ('皇冠', 0.9917600154876709),\n",
              " ('赵县', 0.9893616437911987),\n",
              " ('秋月', 0.9893259406089783),\n",
              " ('锦丰', 0.9868800640106201),\n",
              " ('河北', 0.9868581891059875),\n",
              " ('两', 0.9863934516906738)]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Find the most similar words to \"king\" based on word vectors from our trained model\n",
        "w2v_model.wv.most_similar('鸭梨')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9q5d9jf8dec"
      },
      "outputs": [],
      "source": [
        "words = set(w2v_model.wv.index_to_key )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpTqq2s71EuB"
      },
      "outputs": [],
      "source": [
        "words = set(w2v_model.wv.index_to_key )\n",
        "X_train_vect = [np.array([w2v_model.wv[i] for i in ls if i in words])\n",
        "                         for ls in X_train]\n",
        "X_test_vect = [np.array([w2v_model.wv[i] for i in ls if i in words])\n",
        "                         for ls in X_val]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15URs_GH9ydj"
      },
      "outputs": [],
      "source": [
        "# Calculate the maximum number of words in any sentence to ensure consistent array shape\n",
        "max_length = max(len(ls) for ls in X_train)\n",
        "\n",
        "def pad_vectors(sentences, words, max_length):\n",
        "    padded_vectors = []\n",
        "    for sentence in sentences:\n",
        "        sentence_vectors = [w2v_model.wv[word] for word in sentence if word in words]\n",
        "        # Pad with zero vectors if sentence is shorter than max_length\n",
        "        if len(sentence_vectors) < max_length:\n",
        "            sentence_vectors += [np.zeros(w2v_model.vector_size)] * (max_length - len(sentence_vectors))\n",
        "        padded_vectors.append(sentence_vectors)\n",
        "    return np.array(padded_vectors)\n",
        "\n",
        "X_train_vect = pad_vectors(X_train, words, max_length)\n",
        "X_test_vect = pad_vectors(X_val, words, max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjSVw16c978C"
      },
      "outputs": [],
      "source": [
        "# Compute sentence vectors by averaging the word vectors for the words contained in the sentence\n",
        "X_train_vect_avg = []\n",
        "for v in X_train_vect:\n",
        "    if v.size:\n",
        "        X_train_vect_avg.append(v.mean(axis=0))\n",
        "    else:\n",
        "        X_train_vect_avg.append(np.zeros(100, dtype=float))\n",
        "\n",
        "X_test_vect_avg = []\n",
        "for v in X_test_vect:\n",
        "    if v.size:\n",
        "        X_test_vect_avg.append(v.mean(axis=0))\n",
        "    else:\n",
        "        X_test_vect_avg.append(np.zeros(100, dtype=float))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmOyJ3jnczY4"
      },
      "source": [
        "# ML configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4h_uZ5KgzfIF"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression,LogisticRegressionCV\n",
        "from sklearn.utils import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from keras import models,layers,optimizers,losses,metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxRhq0mOzbgM"
      },
      "source": [
        "# Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLgRe6WH0-fn"
      },
      "outputs": [],
      "source": [
        "#Instantiate\n",
        "model = LogisticRegressionCV(class_weight='balanced')\n",
        "\n",
        "#Fit\n",
        "model.fit(X_train_vect_avg, y_train)\n",
        "\n",
        "#Predict\n",
        "predictions = model.predict(X_test_vect_avg)\n",
        "\n",
        "#Assess\n",
        "standard_accuracy = accuracy_score(predictions,y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEzmqecnCYQu",
        "outputId": "2968c9e2-3df4-4208-8f7b-be045fb5730c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9538296349319971\n"
          ]
        }
      ],
      "source": [
        "print(standard_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pUQgAKFyn05",
        "outputId": "ea0d5ca2-a2da-4dcc-9fe4-467d12475c6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.92      0.97      0.94       975\n",
            "           3       0.97      0.93      0.95       955\n",
            "           2       0.98      0.96      0.97       864\n",
            "\n",
            "    accuracy                           0.95      2794\n",
            "   macro avg       0.96      0.95      0.95      2794\n",
            "weighted avg       0.95      0.95      0.95      2794\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_val,predictions,labels=y.unique()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8PdtBaCGYEO"
      },
      "source": [
        "save the model into local memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "mqabTTOrf9G0",
        "outputId": "b723fddc-5575-4acd-8b48-7fcebead9556"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'LogisticRegressionCV' object has no attribute 'save'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-c9cb75c005c7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'LogisticRegressionCV' object has no attribute 'save'"
          ]
        }
      ],
      "source": [
        "model.save('/content/drive/My Drive/.model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxgQ3I_ix2ef"
      },
      "source": [
        "Multi-lablel classification with logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrM37ZXQx9-w"
      },
      "outputs": [],
      "source": [
        "clf = LogisticRegressionCV(cv=5,penalty= 'elasticnet', solver= 'saga', l1_ratios=[0.1,0.5], random_state=90393).fit(X_train_vect_avg, y_train)\n",
        "y_pred= clf.predict(X_test_vect_avg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Az1sWJa0yPJ9"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test,y_pred,labels=df['IN code'].unique()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kw0VEw7AzDhR"
      },
      "source": [
        "#SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WV2En-s_3rdV"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC,LinearSVC\n",
        "import sklearn.model_selection as model_selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d80Ri-NyzGBZ"
      },
      "outputs": [],
      "source": [
        "# Build model and test for prediction\n",
        "svm = LinearSVC()\n",
        "rbf = SVC(kernel='rbf', gamma=0.5, C=0.1).fit(X_train_vect_avg, y_train)\n",
        "poly = SVC(kernel='poly', degree=3, C=1).fit(X_train_vect_avg, y_train)\n",
        "poly_pred = poly.predict(X_test_vect_avg)\n",
        "rbf_pred = rbf.predict(X_test_vect_avg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0n_0lc2t3Nb8"
      },
      "outputs": [],
      "source": [
        "poly_accuracy = accuracy_score(y_test, poly_pred)\n",
        "poly_f1 = f1_score(y_test, poly_pred, average='weighted')\n",
        "print('Accuracy (Polynomial Kernel): ', \"%.2f\" % (poly_accuracy*100))\n",
        "print('F1 (Polynomial Kernel): ', \"%.2f\" % (poly_f1*100))\n",
        "\n",
        "rbf_accuracy = accuracy_score(y_test, rbf_pred)\n",
        "rbf_f1 = f1_score(y_test, rbf_pred, average='weighted')\n",
        "print('Accuracy (RBF Kernel): ', \"%.2f\" % (rbf_accuracy*100))\n",
        "print('F1 (RBF Kernel): ', \"%.2f\" % (rbf_f1*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrmFNmoU0zwH"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test,poly_pred,labels=df['IN code'].unique()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CP70h9lA04PI"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test,rbf_pred,labels=df['IN code'].unique()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmL2RBQd_A6O"
      },
      "source": [
        "# Random Forest ensemble tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6iAgdI-_AE0"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8zESzjc_RSZ"
      },
      "source": [
        "New version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5woUydMS-DeE"
      },
      "outputs": [],
      "source": [
        "# Instantiate and fit a basic Random Forest model on top of the vectors\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf_model = rf.fit(X_train_vect_avg, y_train.values.ravel())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KypMy_C_OdD"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score\n",
        "# Use the trained model to make predictions on the test data\n",
        "y_pred = rf_model.predict(X_test_vect_avg)\n",
        "f1_score = metrics.f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "print('f1: {} / Accuracy: {}'.format(\n",
        "    round(f1_score,10), round((y_pred==y_test).sum()/len(y_pred), 3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K11rHuvvbwl"
      },
      "source": [
        "old version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FF9ND0QLAAvW"
      },
      "outputs": [],
      "source": [
        "model = RandomForestClassifier(n_estimators=10000, # num of trees\n",
        "                               random_state=42) #randomness control\n",
        "#Fit\n",
        "model.fit(X_train_vect_avg, y_train)\n",
        "\n",
        "#Predict\n",
        "test_predictions = model.predict(X_test)\n",
        "train_predictions = model.predict(X_train)\n",
        "\n",
        "#Assess\n",
        "standard_accuracy_test = accuracy_score(test_predictions,y_test)\n",
        "standard_accuracy_train = accuracy_score(train_predictions,y_train)\n",
        "print(standard_accuracy_test)\n",
        "print(standard_accuracy_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0ztY-sMX63N"
      },
      "outputs": [],
      "source": [
        "print(metrics.classification_report(y_test, predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFZSxBFoD1Pf"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87htnwVWp2YH"
      },
      "source": [
        "## Logistic Regression with pre-trained embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80mmKVeSYU2T"
      },
      "source": [
        "Per inspection, we found that self-trained word embedding yields pretty good result. Hwoever, we msut take cosnideration of the overfitting under the context given that our corpus is relatively small (1000 sentences). Therefore, we shoudl also try different methods of using pre-trained vectors of chinese dictionary/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIrcbdYOchIu"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess the corpus\n",
        "with open('/content/sgns.weibo.bigram-char', 'r', encoding='utf-8') as f:\n",
        "    corpus = f.readlines()\n",
        "\n",
        "# Tokenize each sentence in the corpus\n",
        "corpus_tokenized = [word_tokenize(sentence.lower()) for sentence in corpus]\n",
        "\n",
        "# Train Word2Vec model\n",
        "embedding_dim = 100  # You can set the embedding dimension as needed\n",
        "word2vec_model = Word2Vec(sentences=corpus_tokenized, vector_size=embedding_dim, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Save the model for future use\n",
        "word2vec_model.save('word2vec.model')\n",
        "\n",
        "#Represent first 50 of the words that our Word2Vec model learned a vector for.\n",
        "w2v_model.wv.index_to_key[:50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlN3QCQ9dMX9"
      },
      "outputs": [],
      "source": [
        "# Find the most similar words to \"king\" based on word vectors from our trained model\n",
        "word2vec_model.wv.most_similar('苹果')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHe7gUCRdZFi"
      },
      "outputs": [],
      "source": [
        "words = set(word2vec_model.wv.index_to_key )\n",
        "words = set(word2vec_model.wv.index_to_key )\n",
        "X_train_vect = [np.array([word2vec_model.wv[i] for i in ls if i in words])\n",
        "                         for ls in X_train]\n",
        "X_test_vect = [np.array([word2vec_model.wv[i] for i in ls if i in words])\n",
        "                         for ls in X_val]\n",
        "# Calculate the maximum number of words in any sentence to ensure consistent array shape\n",
        "max_length = max(len(ls) for ls in X_train)\n",
        "\n",
        "def pad_vectors(sentences, words, max_length):\n",
        "    padded_vectors = []\n",
        "    for sentence in sentences:\n",
        "        sentence_vectors = [word2vec_model.wv[word] for word in sentence if word in words]\n",
        "        # Pad with zero vectors if sentence is shorter than max_length\n",
        "        if len(sentence_vectors) < max_length:\n",
        "            sentence_vectors += [np.zeros(word2vec_model.vector_size)] * (max_length - len(sentence_vectors))\n",
        "        padded_vectors.append(sentence_vectors)\n",
        "    return np.array(padded_vectors)\n",
        "\n",
        "X_train_vect = pad_vectors(X_train, words, max_length)\n",
        "X_test_vect = pad_vectors(X_val, words, max_length)\n",
        "# Compute sentence vectors by averaging the word vectors for the words contained in the sentence\n",
        "X_train_vect_avg = []\n",
        "for v in X_train_vect:\n",
        "    if v.size:\n",
        "        X_train_vect_avg.append(v.mean(axis=0))\n",
        "    else:\n",
        "        X_train_vect_avg.append(np.zeros(100, dtype=float))\n",
        "\n",
        "X_test_vect_avg = []\n",
        "for v in X_test_vect:\n",
        "    if v.size:\n",
        "        X_test_vect_avg.append(v.mean(axis=0))\n",
        "    else:\n",
        "        X_test_vect_avg.append(np.zeros(100, dtype=float))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "084kulzGdpud"
      },
      "outputs": [],
      "source": [
        "#Instantiate\n",
        "model = LogisticRegressionCV(class_weight='balanced')\n",
        "\n",
        "#Fit\n",
        "model.fit(X_train_vect_avg, y_train)\n",
        "\n",
        "#Predict\n",
        "predictions = model.predict(X_test_vect_avg)\n",
        "\n",
        "#Assess\n",
        "standard_accuracy = accuracy_score(predictions,y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZrobOnteEwU"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_val,predictions,labels=y.unique()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeZl-bpDnfqd"
      },
      "source": [
        "It turns out that the model perofrm poorly absed on the pre-trained word embeddings. Many reasons could induce to such result, namely one of them being the improper use of the corpus for the pre-training. Other reasons,like using the improper ML models, would also inccur this result. But for now let's stay with the self trained embedding vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-WZ7IKeHZxN"
      },
      "source": [
        "# Dense layer network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaMmp4quBhez"
      },
      "source": [
        "##RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSe16xWCG94x"
      },
      "outputs": [],
      "source": [
        "#Make the necessary imports\n",
        "import tarfile\n",
        "import wget\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from zipfile import ZipFile\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.initializers import Constant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCVKhMMTFaEw",
        "outputId": "09ba26f7-4dd4-4b8b-d905-d3d65e54b267"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "nltk.download('wordnet')\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsZ2r3ugFfwi",
        "outputId": "2015a703-f924-40e0-91c0-d3ee57225a6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAykCl4DF8t1"
      },
      "source": [
        "Tokenizing and Padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ublh7MVzLmyz"
      },
      "source": [
        "### First, start with a baseline model using pre-trained embeddings without parameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4DiZE3ubErSS"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess the pre-trained corpus\n",
        "with open(w2v_model2, 'r', encoding='utf-8') as f:\n",
        "    corpus = f.readlines()\n",
        "\n",
        "# Tokenize each sentence in the corpus\n",
        "corpus_tokenized = [word_tokenize(sentence.lower()) for sentence in corpus]\n",
        "\n",
        "# Train Word2Vec model\n",
        "embedding_dim = 300  # You can set the embedding dimension as needed\n",
        "word2vec_model = Word2Vec(sentences=corpus_tokenized, vector_size=embedding_dim, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Save the model for future use\n",
        "word2vec_model.save('word2vec.model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kO2bI9HzpB0"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained Word2Vec model.\n",
        "word2vec_model = Word2Vec.load(\"word2vec.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zQ6LisoG_FD"
      },
      "outputs": [],
      "source": [
        "word2vec_model.wv.most_similar('嘎啦')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goG1gIV_MkPh"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHmF8x2x_Wj0"
      },
      "outputs": [],
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMVzdYQHFXTb"
      },
      "outputs": [],
      "source": [
        "# Load the trained Word2Vec model\n",
        "#word2vec_model = Word2Vec.load('word2vec.model')\n",
        "\n",
        "embedding_dim = 300\n",
        "texts = df['description_token'].tolist()\n",
        "\n",
        "# Tokenize the input sequences (fit a tokenizer)\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Convert texts to sequences of word indices\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Padding sequences to ensure uniform input size for the LSTM model\n",
        "max_length = max([len(seq) for seq in sequences])\n",
        "X = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "# Preparing the embedding matrix from the Word2Vec model\n",
        "vocab_size = len(word_index) + 1  # Account for padding token\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if word in word2vec_model.wv:\n",
        "        embedding_matrix[i] = word2vec_model.wv[word]\n",
        "    else:\n",
        "        embedding_matrix[i] = np.random.normal(size=(embedding_dim,))  # Handle OOV words\n",
        "\n",
        "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYYFnDxMI6fh",
        "outputId": "3abaa871-293c-4cba-adad-a95495ef362b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 30ms/step - accuracy: 0.3856 - loss: 1.0717 - val_accuracy: 0.4496 - val_loss: 0.9961\n",
            "Epoch 2/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 29ms/step - accuracy: 0.4930 - loss: 0.9645 - val_accuracy: 0.4726 - val_loss: 0.9593\n",
            "Epoch 3/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 29ms/step - accuracy: 0.5111 - loss: 0.9189 - val_accuracy: 0.4743 - val_loss: 0.9611\n",
            "Epoch 4/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 29ms/step - accuracy: 0.5239 - loss: 0.8908 - val_accuracy: 0.5761 - val_loss: 0.9544\n",
            "Epoch 5/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 29ms/step - accuracy: 0.5435 - loss: 0.8686 - val_accuracy: 0.5084 - val_loss: 0.9461\n",
            "Epoch 6/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 29ms/step - accuracy: 0.5828 - loss: 0.8277 - val_accuracy: 0.5203 - val_loss: 0.9522\n",
            "Epoch 7/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 29ms/step - accuracy: 0.6037 - loss: 0.8062 - val_accuracy: 0.5922 - val_loss: 0.9371\n",
            "Epoch 8/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 29ms/step - accuracy: 0.6567 - loss: 0.7578 - val_accuracy: 0.6250 - val_loss: 0.9029\n",
            "Epoch 9/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 29ms/step - accuracy: 0.7138 - loss: 0.6559 - val_accuracy: 0.6900 - val_loss: 0.7706\n",
            "Epoch 10/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 29ms/step - accuracy: 0.7461 - loss: 0.6050 - val_accuracy: 0.7240 - val_loss: 0.7226\n",
            "Epoch 11/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 29ms/step - accuracy: 0.7753 - loss: 0.5452 - val_accuracy: 0.7446 - val_loss: 0.6988\n",
            "Epoch 12/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 29ms/step - accuracy: 0.8035 - loss: 0.4970 - val_accuracy: 0.7530 - val_loss: 0.6546\n",
            "Epoch 13/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 29ms/step - accuracy: 0.8096 - loss: 0.4724 - val_accuracy: 0.7715 - val_loss: 0.6551\n",
            "Epoch 14/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 29ms/step - accuracy: 0.8248 - loss: 0.4496 - val_accuracy: 0.7619 - val_loss: 0.6412\n",
            "Epoch 15/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 28ms/step - accuracy: 0.8342 - loss: 0.4261 - val_accuracy: 0.7912 - val_loss: 0.6226\n",
            "Epoch 16/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 28ms/step - accuracy: 0.8374 - loss: 0.4168 - val_accuracy: 0.7939 - val_loss: 0.5930\n",
            "Epoch 17/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 29ms/step - accuracy: 0.8566 - loss: 0.3662 - val_accuracy: 0.8037 - val_loss: 0.5613\n",
            "Epoch 18/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 29ms/step - accuracy: 0.8564 - loss: 0.3694 - val_accuracy: 0.8129 - val_loss: 0.5530\n",
            "Epoch 19/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 29ms/step - accuracy: 0.8699 - loss: 0.3355 - val_accuracy: 0.8037 - val_loss: 0.5667\n",
            "Epoch 20/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 29ms/step - accuracy: 0.8745 - loss: 0.3234 - val_accuracy: 0.8118 - val_loss: 0.5689\n",
            "Epoch 21/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 29ms/step - accuracy: 0.8805 - loss: 0.3207 - val_accuracy: 0.8246 - val_loss: 0.5476\n",
            "Epoch 22/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 28ms/step - accuracy: 0.8844 - loss: 0.3064 - val_accuracy: 0.8258 - val_loss: 0.5343\n",
            "Epoch 23/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 29ms/step - accuracy: 0.8874 - loss: 0.3038 - val_accuracy: 0.8010 - val_loss: 0.5968\n",
            "Epoch 24/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 29ms/step - accuracy: 0.8885 - loss: 0.2980 - val_accuracy: 0.8210 - val_loss: 0.5606\n",
            "Epoch 25/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 29ms/step - accuracy: 0.8918 - loss: 0.2861 - val_accuracy: 0.8350 - val_loss: 0.5320\n",
            "Epoch 26/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 30ms/step - accuracy: 0.9067 - loss: 0.2539 - val_accuracy: 0.8326 - val_loss: 0.5654\n",
            "Epoch 27/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 30ms/step - accuracy: 0.8951 - loss: 0.2744 - val_accuracy: 0.8422 - val_loss: 0.5286\n",
            "Epoch 28/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 30ms/step - accuracy: 0.9090 - loss: 0.2578 - val_accuracy: 0.8231 - val_loss: 0.5477\n",
            "Epoch 29/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 29ms/step - accuracy: 0.8984 - loss: 0.2602 - val_accuracy: 0.8323 - val_loss: 0.5580\n",
            "Epoch 30/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 29ms/step - accuracy: 0.9027 - loss: 0.2608 - val_accuracy: 0.8428 - val_loss: 0.5274\n",
            "Epoch 31/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 29ms/step - accuracy: 0.9031 - loss: 0.2587 - val_accuracy: 0.8467 - val_loss: 0.5226\n",
            "Epoch 32/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 29ms/step - accuracy: 0.9107 - loss: 0.2441 - val_accuracy: 0.8377 - val_loss: 0.5544\n",
            "Epoch 33/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 29ms/step - accuracy: 0.9048 - loss: 0.2409 - val_accuracy: 0.8210 - val_loss: 0.5680\n",
            "Epoch 34/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 28ms/step - accuracy: 0.9136 - loss: 0.2293 - val_accuracy: 0.8467 - val_loss: 0.5124\n",
            "Epoch 35/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 29ms/step - accuracy: 0.9130 - loss: 0.2337 - val_accuracy: 0.8461 - val_loss: 0.5321\n",
            "Epoch 36/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 29ms/step - accuracy: 0.9214 - loss: 0.2131 - val_accuracy: 0.8401 - val_loss: 0.5721\n",
            "Epoch 37/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 29ms/step - accuracy: 0.9185 - loss: 0.2253 - val_accuracy: 0.8479 - val_loss: 0.5284\n",
            "Epoch 38/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 29ms/step - accuracy: 0.9161 - loss: 0.2215 - val_accuracy: 0.8490 - val_loss: 0.5165\n",
            "Epoch 39/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 28ms/step - accuracy: 0.9155 - loss: 0.2167 - val_accuracy: 0.8452 - val_loss: 0.5207\n",
            "Epoch 40/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 29ms/step - accuracy: 0.9166 - loss: 0.2191 - val_accuracy: 0.8493 - val_loss: 0.5219\n",
            "Epoch 41/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 29ms/step - accuracy: 0.9251 - loss: 0.2006 - val_accuracy: 0.8484 - val_loss: 0.5247\n",
            "Epoch 42/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 29ms/step - accuracy: 0.9240 - loss: 0.1996 - val_accuracy: 0.8499 - val_loss: 0.5287\n",
            "Epoch 43/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 30ms/step - accuracy: 0.9237 - loss: 0.2006 - val_accuracy: 0.8458 - val_loss: 0.5129\n",
            "Epoch 44/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 30ms/step - accuracy: 0.9280 - loss: 0.1977 - val_accuracy: 0.8520 - val_loss: 0.4945\n",
            "Epoch 45/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 28ms/step - accuracy: 0.9265 - loss: 0.1946 - val_accuracy: 0.8452 - val_loss: 0.5261\n",
            "Epoch 46/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 29ms/step - accuracy: 0.9254 - loss: 0.2026 - val_accuracy: 0.8440 - val_loss: 0.5556\n",
            "Epoch 47/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 29ms/step - accuracy: 0.9295 - loss: 0.1900 - val_accuracy: 0.8532 - val_loss: 0.5126\n",
            "Epoch 48/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 30ms/step - accuracy: 0.9340 - loss: 0.1888 - val_accuracy: 0.8559 - val_loss: 0.5039\n",
            "Epoch 49/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.9343 - loss: 0.1918 - val_accuracy: 0.8568 - val_loss: 0.5099\n",
            "Epoch 50/50\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - accuracy: 0.9264 - loss: 0.1922 - val_accuracy: 0.8502 - val_loss: 0.5205\n"
          ]
        }
      ],
      "source": [
        "# Assume df['IN code'] contains the target categorical labels\n",
        "\n",
        "# Step 1: Encode labels into integers\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(df['IN code'])  # Convert categorical labels to integers\n",
        "\n",
        "# Step 2: One-hot encode the labels (fix the shape issue)\n",
        "num_classes = 3  # You already know there are 3 unique categories\n",
        "y_one_hot = to_categorical(y_encoded, num_classes=num_classes)\n",
        "\n",
        "# Step 3: Split the data into training and validation sets\n",
        "# Assuming `X` contains your input features (tokenized and padded sequences)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_one_hot, test_size=0.3, random_state=42)\n",
        "X_val,X_test,y_val,y_test = train_test_split(X_val,y_val, test_size=0.2, random_state=42)\n",
        "# Now define and compile your model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Step 4: Build the LSTM model\n",
        "model = Sequential()\n",
        "\n",
        "# Add embedding layer (make sure vocab_size and embedding_matrix are defined)\n",
        "model.add(Embedding(input_dim=vocab_size,\n",
        "                    output_dim=embedding_dim,\n",
        "                    weights=[embedding_matrix],\n",
        "                    input_length=max_length,\n",
        "                    trainable=False))\n",
        "\n",
        "# Add LSTM layer\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "\n",
        "# Add output layer (number of units must match the number of categories)\n",
        "model.add(Dense(num_classes, activation='softmax'))  # Softmax for multi-class classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 5: Train the model (fit the model)\n",
        "history = model.fit(X_train, y_train,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    epochs=50,\n",
        "                    batch_size=32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QNnB7fxP2Y_4",
        "outputId": "911edf3c-015e-440b-c618-41e69e41e9fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.3412 - loss: 1.0984 - val_accuracy: 0.4096 - val_loss: 1.0915\n",
            "Epoch 2/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - accuracy: 0.3843 - loss: 1.0756 - val_accuracy: 0.4141 - val_loss: 1.0224\n",
            "Epoch 3/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 32ms/step - accuracy: 0.4417 - loss: 1.0206 - val_accuracy: 0.4899 - val_loss: 1.0010\n",
            "Epoch 4/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.4752 - loss: 0.9893 - val_accuracy: 0.4526 - val_loss: 0.9843\n",
            "Epoch 5/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 30ms/step - accuracy: 0.4946 - loss: 0.9570 - val_accuracy: 0.4645 - val_loss: 0.9733\n",
            "Epoch 6/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 30ms/step - accuracy: 0.4941 - loss: 0.9535 - val_accuracy: 0.4684 - val_loss: 0.9663\n",
            "Epoch 7/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.5130 - loss: 0.9349 - val_accuracy: 0.4630 - val_loss: 0.9633\n",
            "Epoch 8/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 30ms/step - accuracy: 0.5219 - loss: 0.9203 - val_accuracy: 0.4743 - val_loss: 0.9580\n",
            "Epoch 9/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - accuracy: 0.5398 - loss: 0.9071 - val_accuracy: 0.4788 - val_loss: 0.9610\n",
            "Epoch 10/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - accuracy: 0.5302 - loss: 0.8942 - val_accuracy: 0.5280 - val_loss: 0.9507\n",
            "Epoch 11/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 32ms/step - accuracy: 0.5449 - loss: 0.8912 - val_accuracy: 0.4827 - val_loss: 0.9590\n",
            "Epoch 12/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - accuracy: 0.5612 - loss: 0.8747 - val_accuracy: 0.4806 - val_loss: 0.9543\n",
            "Epoch 13/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.5571 - loss: 0.8809 - val_accuracy: 0.5233 - val_loss: 0.9567\n",
            "Epoch 14/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.5659 - loss: 0.8595 - val_accuracy: 0.5224 - val_loss: 0.9500\n",
            "Epoch 15/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.5740 - loss: 0.8573 - val_accuracy: 0.5507 - val_loss: 0.9483\n",
            "Epoch 16/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 30ms/step - accuracy: 0.5656 - loss: 0.8539 - val_accuracy: 0.4940 - val_loss: 0.9570\n",
            "Epoch 17/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 30ms/step - accuracy: 0.5749 - loss: 0.8524 - val_accuracy: 0.4913 - val_loss: 0.9526\n",
            "Epoch 18/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 30ms/step - accuracy: 0.5636 - loss: 0.8429 - val_accuracy: 0.5531 - val_loss: 0.9532\n",
            "Epoch 19/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 30ms/step - accuracy: 0.5818 - loss: 0.8407 - val_accuracy: 0.4857 - val_loss: 0.9579\n",
            "Epoch 20/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 30ms/step - accuracy: 0.5940 - loss: 0.8261 - val_accuracy: 0.5400 - val_loss: 0.9524\n",
            "Epoch 21/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.5927 - loss: 0.8235 - val_accuracy: 0.5650 - val_loss: 0.9599\n",
            "Epoch 22/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - accuracy: 0.5925 - loss: 0.8288 - val_accuracy: 0.6005 - val_loss: 0.9531\n",
            "Epoch 23/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - accuracy: 0.6026 - loss: 0.8187 - val_accuracy: 0.5436 - val_loss: 0.9574\n",
            "Epoch 24/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 30ms/step - accuracy: 0.6126 - loss: 0.8073 - val_accuracy: 0.4931 - val_loss: 0.9589\n",
            "Epoch 25/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.6083 - loss: 0.7904 - val_accuracy: 0.5015 - val_loss: 0.9703\n",
            "Epoch 26/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 30ms/step - accuracy: 0.6064 - loss: 0.7946 - val_accuracy: 0.5898 - val_loss: 0.9558\n",
            "Epoch 27/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 30ms/step - accuracy: 0.6270 - loss: 0.7922 - val_accuracy: 0.5984 - val_loss: 0.9670\n",
            "Epoch 28/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 30ms/step - accuracy: 0.6145 - loss: 0.8023 - val_accuracy: 0.6071 - val_loss: 0.9717\n",
            "Epoch 29/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 30ms/step - accuracy: 0.6335 - loss: 0.7752 - val_accuracy: 0.5892 - val_loss: 0.9790\n",
            "Epoch 30/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 30ms/step - accuracy: 0.6478 - loss: 0.7823 - val_accuracy: 0.5549 - val_loss: 0.9803\n",
            "Epoch 31/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.6395 - loss: 0.7780 - val_accuracy: 0.6342 - val_loss: 0.9765\n",
            "Epoch 32/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 32ms/step - accuracy: 0.6506 - loss: 0.7804 - val_accuracy: 0.6513 - val_loss: 0.9835\n",
            "Epoch 33/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 32ms/step - accuracy: 0.6578 - loss: 0.7768 - val_accuracy: 0.5710 - val_loss: 0.9843\n",
            "Epoch 34/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.6717 - loss: 0.7600 - val_accuracy: 0.6172 - val_loss: 0.9842\n",
            "Epoch 35/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.6801 - loss: 0.7536 - val_accuracy: 0.6721 - val_loss: 0.9594\n",
            "Epoch 36/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 29ms/step - accuracy: 0.7072 - loss: 0.7259 - val_accuracy: 0.6134 - val_loss: 0.9348\n",
            "Epoch 37/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.7273 - loss: 0.6696 - val_accuracy: 0.7053 - val_loss: 0.8439\n",
            "Epoch 38/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.7387 - loss: 0.6489 - val_accuracy: 0.7202 - val_loss: 0.7972\n",
            "Epoch 39/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.7511 - loss: 0.6239 - val_accuracy: 0.6820 - val_loss: 0.8077\n",
            "Epoch 40/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 30ms/step - accuracy: 0.7644 - loss: 0.5900 - val_accuracy: 0.7139 - val_loss: 0.7733\n",
            "Epoch 41/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.7571 - loss: 0.5973 - val_accuracy: 0.7008 - val_loss: 0.8055\n",
            "Epoch 42/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.7579 - loss: 0.5932 - val_accuracy: 0.7342 - val_loss: 0.7391\n",
            "Epoch 43/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 30ms/step - accuracy: 0.7662 - loss: 0.5825 - val_accuracy: 0.7413 - val_loss: 0.7373\n",
            "Epoch 44/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 30ms/step - accuracy: 0.7740 - loss: 0.5670 - val_accuracy: 0.7148 - val_loss: 0.7604\n",
            "Epoch 45/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 30ms/step - accuracy: 0.7706 - loss: 0.5563 - val_accuracy: 0.7360 - val_loss: 0.7320\n",
            "Epoch 46/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 30ms/step - accuracy: 0.7852 - loss: 0.5521 - val_accuracy: 0.7151 - val_loss: 0.7468\n",
            "Epoch 47/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 30ms/step - accuracy: 0.7679 - loss: 0.5692 - val_accuracy: 0.7378 - val_loss: 0.7201\n",
            "Epoch 48/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.7751 - loss: 0.5594 - val_accuracy: 0.7246 - val_loss: 0.7292\n",
            "Epoch 49/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.7807 - loss: 0.5514 - val_accuracy: 0.7485 - val_loss: 0.7125\n",
            "Epoch 50/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.7833 - loss: 0.5357 - val_accuracy: 0.7324 - val_loss: 0.7220\n",
            "Epoch 51/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 30ms/step - accuracy: 0.7800 - loss: 0.5407 - val_accuracy: 0.7094 - val_loss: 0.7539\n",
            "Epoch 52/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.7870 - loss: 0.5272 - val_accuracy: 0.7297 - val_loss: 0.7201\n",
            "Epoch 53/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.7814 - loss: 0.5500 - val_accuracy: 0.7312 - val_loss: 0.7195\n",
            "Epoch 54/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - accuracy: 0.7935 - loss: 0.5211 - val_accuracy: 0.7381 - val_loss: 0.7190\n",
            "Epoch 55/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.7870 - loss: 0.5402 - val_accuracy: 0.7545 - val_loss: 0.7034\n",
            "Epoch 56/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.7934 - loss: 0.5105 - val_accuracy: 0.7413 - val_loss: 0.7121\n",
            "Epoch 57/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.7966 - loss: 0.5107 - val_accuracy: 0.7405 - val_loss: 0.7021\n",
            "Epoch 58/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - accuracy: 0.7921 - loss: 0.5203 - val_accuracy: 0.7324 - val_loss: 0.7174\n",
            "Epoch 59/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 30ms/step - accuracy: 0.7900 - loss: 0.5227 - val_accuracy: 0.7479 - val_loss: 0.7016\n",
            "Epoch 60/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.8028 - loss: 0.4970 - val_accuracy: 0.7303 - val_loss: 0.7274\n",
            "Epoch 61/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 30ms/step - accuracy: 0.8038 - loss: 0.4918 - val_accuracy: 0.7288 - val_loss: 0.7420\n",
            "Epoch 62/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 30ms/step - accuracy: 0.7978 - loss: 0.5002 - val_accuracy: 0.7581 - val_loss: 0.7007\n",
            "Epoch 63/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 30ms/step - accuracy: 0.7993 - loss: 0.4967 - val_accuracy: 0.7384 - val_loss: 0.7088\n",
            "Epoch 64/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - accuracy: 0.7984 - loss: 0.4921 - val_accuracy: 0.7273 - val_loss: 0.7473\n",
            "Epoch 65/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 32ms/step - accuracy: 0.7921 - loss: 0.5099 - val_accuracy: 0.7449 - val_loss: 0.7277\n",
            "Epoch 66/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - accuracy: 0.7971 - loss: 0.4929 - val_accuracy: 0.7500 - val_loss: 0.7100\n",
            "Epoch 67/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.7976 - loss: 0.5048 - val_accuracy: 0.7548 - val_loss: 0.7003\n",
            "Epoch 68/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - accuracy: 0.8102 - loss: 0.4766 - val_accuracy: 0.7598 - val_loss: 0.7071\n",
            "Epoch 69/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - accuracy: 0.8028 - loss: 0.4775 - val_accuracy: 0.7548 - val_loss: 0.7109\n",
            "Epoch 70/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.8030 - loss: 0.4872 - val_accuracy: 0.7551 - val_loss: 0.6939\n",
            "Epoch 71/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 30ms/step - accuracy: 0.8023 - loss: 0.4762 - val_accuracy: 0.7539 - val_loss: 0.6897\n",
            "Epoch 72/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - accuracy: 0.8123 - loss: 0.4768 - val_accuracy: 0.7461 - val_loss: 0.6990\n",
            "Epoch 73/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - accuracy: 0.8114 - loss: 0.4665 - val_accuracy: 0.7512 - val_loss: 0.7067\n",
            "Epoch 74/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.8072 - loss: 0.4767 - val_accuracy: 0.7601 - val_loss: 0.7014\n",
            "Epoch 75/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 30ms/step - accuracy: 0.8119 - loss: 0.4759 - val_accuracy: 0.7425 - val_loss: 0.7230\n",
            "Epoch 76/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.8010 - loss: 0.4721 - val_accuracy: 0.7515 - val_loss: 0.7201\n",
            "Epoch 77/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - accuracy: 0.8128 - loss: 0.4687 - val_accuracy: 0.7482 - val_loss: 0.7078\n",
            "Epoch 78/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - accuracy: 0.8156 - loss: 0.4586 - val_accuracy: 0.7619 - val_loss: 0.7083\n",
            "Epoch 79/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 30ms/step - accuracy: 0.8189 - loss: 0.4541 - val_accuracy: 0.7637 - val_loss: 0.7047\n",
            "Epoch 80/100\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - accuracy: 0.8142 - loss: 0.4629 - val_accuracy: 0.7584 - val_loss: 0.6963\n",
            "Epoch 81/100\n",
            "\u001b[1m302/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8186 - loss: 0.4527"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-07f008279cb2>\u001b[0m in \u001b[0;36m<cell line: 43>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Step 5: Train the model (fit the model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m history = model.fit(X_train, y_train,\n\u001b[0m\u001b[1;32m     44\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pythonify_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1553\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#change the learning rate to 0.001\n",
        "\n",
        "# Step 1: Encode labels into integers\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(df['IN code'])  # Convert categorical labels to integers\n",
        "\n",
        "# Step 2: One-hot encode the labels (fix the shape issue)\n",
        "num_classes = 3  # You already know there are 3 unique categories\n",
        "y_one_hot = to_categorical(y_encoded, num_classes=num_classes)\n",
        "\n",
        "# Step 3: Split the data into training and validation sets\n",
        "# Assuming `X` contains your input features (tokenized and padded sequences)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_one_hot, test_size=0.3, random_state=42)\n",
        "X_val,X_test,y_val,y_test = train_test_split(X_val,y_val, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Step 4: Build the LSTM model\n",
        "model = Sequential()\n",
        "\n",
        "# Add embedding layer (make sure vocab_size and embedding_matrix are defined)\n",
        "model.add(Embedding(input_dim=vocab_size,\n",
        "                    output_dim=embedding_dim,\n",
        "                    weights=[embedding_matrix],\n",
        "                    input_length=max_length,\n",
        "                    trainable=False))\n",
        "\n",
        "# Add LSTM layer\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "\n",
        "# Add output layer (number of units must match the number of categories)\n",
        "model.add(Dense(num_classes, activation='softmax'))  # Softmax for multi-class classification\n",
        "\n",
        "# Compile the model\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Define the optimizer with a specific learning rate\n",
        "optimizer = Adam(learning_rate=0.0001)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 5: Train the model (fit the model)\n",
        "history = model.fit(X_train, y_train,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    epochs=100,\n",
        "                    batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIAPL00g7TB8"
      },
      "outputs": [],
      "source": [
        "# Step 1: Encode labels into integers\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(df['IN code'])  # Convert categorical labels to integers\n",
        "\n",
        "# Step 2: One-hot encode the labels (fix the shape issue)\n",
        "num_classes = 3  # You already know there are 3 unique categories\n",
        "y_one_hot = to_categorical(y_encoded, num_classes=num_classes)\n",
        "\n",
        "# Step 3: Split the data into training and validation sets\n",
        "# Assuming `X` contains your input features (tokenized and padded sequences)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_one_hot, test_size=0.3, random_state=42)\n",
        "X_val,X_test,y_val,y_test = train_test_split(X_val,y_val, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Step 4: Build the LSTM model\n",
        "model = Sequential()\n",
        "\n",
        "# Add embedding layer (make sure vocab_size and embedding_matrix are defined)\n",
        "model.add(Embedding(input_dim=vocab_size,\n",
        "                    output_dim=embedding_dim,\n",
        "                    weights=[embedding_matrix],\n",
        "                    input_length=max_length,\n",
        "                    trainable=False))\n",
        "\n",
        "# Add LSTM layer\n",
        "model.add(LSTM(1320, dropout=0.2, recurrent_dropout=0.2,kernel_regularizer=l2(0.001)))\n",
        "\n",
        "# Add output layer (number of units must match the number of categories)\n",
        "model.add(Dense(num_classes, activation='softmax',kernel_regularizer=l2(0.001)))  # Softmax for multi-class classification\n",
        "\n",
        "# Compile the model\n",
        "\n",
        "# Define the optimizer with a specific learning rate\n",
        "optimizer = Adam(learning_rate=0.0001)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 5: Train the model (fit the model)\n",
        "history = model.fit(X_train, y_train,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    epochs=1000,\n",
        "                    batch_size=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIFjVIVlQHOu"
      },
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBfv1xx-Lf2U"
      },
      "outputs": [],
      "source": [
        "# Step 1: Encode labels into integers\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(df['IN code'])  # Convert categorical labels to integers\n",
        "\n",
        "# Step 2: One-hot encode the labels (fix the shape issue)\n",
        "num_classes = 3  # You already know there are 3 unique categories\n",
        "y_one_hot = to_categorical(y_encoded, num_classes=num_classes)\n",
        "\n",
        "# Step 3: Split the data into training and validation sets\n",
        "# Assuming `X` contains your input features (tokenized and padded sequences)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_one_hot, test_size=0.3, random_state=42)\n",
        "X_val,X_test,y_val,y_test = train_test_split(X_val,y_val, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Step 4: Build the LSTM model\n",
        "model = Sequential()\n",
        "\n",
        "# Add embedding layer (make sure vocab_size and embedding_matrix are defined)\n",
        "model.add(Embedding(input_dim=vocab_size,\n",
        "                    output_dim=embedding_dim,\n",
        "                    weights=[embedding_matrix],\n",
        "                    input_length=max_length,\n",
        "                    trainable=False))\n",
        "\n",
        "# Add LSTM layer\n",
        "model.add(LSTM(1024, dropout=0.2, recurrent_dropout=0.2,kernel_regularizer=l2(0.001)))\n",
        "\n",
        "# Add output layer (number of units must match the number of categories)\n",
        "model.add(Dense(num_classes, activation='softmax',kernel_regularizer=l2(0.001)))  # Softmax for multi-class classification\n",
        "\n",
        "# Compile the model\n",
        "\n",
        "# Define the optimizer with a specific learning rate\n",
        "optimizer = Adam(learning_rate=0.0001)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 5: Train the model (fit the model)\n",
        "history = model.fit(X_train, y_train,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    epochs=1000,\n",
        "                    batch_size=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeQjicB_7B7k"
      },
      "source": [
        "Based on val accuracy inspection, I decide to stay with an early stoppoint of epoch at 36."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwuYt3F24tKm",
        "outputId": "fba74c92-33bb-4b84-9ed8-81a3145ecf35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/36\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.7865 - loss: 0.5128 - val_accuracy: 0.9520 - val_loss: 0.1810\n",
            "Epoch 2/36\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 32ms/step - accuracy: 0.9473 - loss: 0.2015 - val_accuracy: 0.9627 - val_loss: 0.1557\n",
            "Epoch 3/36\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.9604 - loss: 0.1660 - val_accuracy: 0.9603 - val_loss: 0.1680\n",
            "Epoch 4/36\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 32ms/step - accuracy: 0.9646 - loss: 0.1494 - val_accuracy: 0.9657 - val_loss: 0.1398\n",
            "Epoch 5/36\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - accuracy: 0.9704 - loss: 0.1257 - val_accuracy: 0.9672 - val_loss: 0.1357\n",
            "Epoch 6/36\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - accuracy: 0.9753 - loss: 0.1105 - val_accuracy: 0.9699 - val_loss: 0.1196\n",
            "Epoch 7/36\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 32ms/step - accuracy: 0.9806 - loss: 0.0885 - val_accuracy: 0.9708 - val_loss: 0.1154\n",
            "Epoch 8/36\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.9787 - loss: 0.0947 - val_accuracy: 0.9732 - val_loss: 0.1171\n",
            "Epoch 9/36\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.9781 - loss: 0.0906 - val_accuracy: 0.9746 - val_loss: 0.1136\n",
            "Epoch 10/36\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - accuracy: 0.9839 - loss: 0.0706 - val_accuracy: 0.9740 - val_loss: 0.1106\n",
            "Epoch 11/36\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.9836 - loss: 0.0732 - val_accuracy: 0.9746 - val_loss: 0.1088\n",
            "Epoch 12/36\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 32ms/step - accuracy: 0.9849 - loss: 0.0657 - val_accuracy: 0.9749 - val_loss: 0.1076\n",
            "Epoch 13/36\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 32ms/step - accuracy: 0.9862 - loss: 0.0628 - val_accuracy: 0.9749 - val_loss: 0.1161\n",
            "Epoch 14/36\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 32ms/step - accuracy: 0.9864 - loss: 0.0562 - val_accuracy: 0.9726 - val_loss: 0.1302\n",
            "Epoch 15/36\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 32ms/step - accuracy: 0.9866 - loss: 0.0597 - val_accuracy: 0.9737 - val_loss: 0.1088\n",
            "Epoch 16/36\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - accuracy: 0.9877 - loss: 0.0492 - val_accuracy: 0.9732 - val_loss: 0.1148\n",
            "Epoch 17/36\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.9900 - loss: 0.0439 - val_accuracy: 0.9723 - val_loss: 0.1177\n",
            "Epoch 18/36\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 32ms/step - accuracy: 0.9904 - loss: 0.0408 - val_accuracy: 0.9752 - val_loss: 0.1162\n",
            "Epoch 19/36\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.9887 - loss: 0.0418 - val_accuracy: 0.9732 - val_loss: 0.1129\n",
            "Epoch 20/36\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - accuracy: 0.9889 - loss: 0.0407 - val_accuracy: 0.9770 - val_loss: 0.1068\n",
            "Epoch 21/36\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 32ms/step - accuracy: 0.9916 - loss: 0.0315 - val_accuracy: 0.9758 - val_loss: 0.1145\n",
            "Epoch 22/36\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - accuracy: 0.9887 - loss: 0.0398 - val_accuracy: 0.9773 - val_loss: 0.1274\n",
            "Epoch 23/36\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - accuracy: 0.9908 - loss: 0.0335 - val_accuracy: 0.9758 - val_loss: 0.1171\n",
            "Epoch 24/36\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 32ms/step - accuracy: 0.9897 - loss: 0.0361 - val_accuracy: 0.9758 - val_loss: 0.1213\n",
            "Epoch 25/36\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - accuracy: 0.9907 - loss: 0.0360 - val_accuracy: 0.9767 - val_loss: 0.1306\n",
            "Epoch 26/36\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.9926 - loss: 0.0273 - val_accuracy: 0.9770 - val_loss: 0.1308\n",
            "Epoch 27/36\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - accuracy: 0.9908 - loss: 0.0287 - val_accuracy: 0.9764 - val_loss: 0.1268\n",
            "Epoch 28/36\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 32ms/step - accuracy: 0.9909 - loss: 0.0281 - val_accuracy: 0.9755 - val_loss: 0.1364\n",
            "Epoch 29/36\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 32ms/step - accuracy: 0.9916 - loss: 0.0241 - val_accuracy: 0.9761 - val_loss: 0.1354\n",
            "Epoch 30/36\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - accuracy: 0.9925 - loss: 0.0252 - val_accuracy: 0.9761 - val_loss: 0.1308\n",
            "Epoch 31/36\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - accuracy: 0.9906 - loss: 0.0337 - val_accuracy: 0.9779 - val_loss: 0.1291\n",
            "Epoch 32/36\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - accuracy: 0.9945 - loss: 0.0217 - val_accuracy: 0.9773 - val_loss: 0.1189\n",
            "Epoch 33/36\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - accuracy: 0.9937 - loss: 0.0240 - val_accuracy: 0.9779 - val_loss: 0.1331\n",
            "Epoch 34/36\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 0.9942 - loss: 0.0208 - val_accuracy: 0.9752 - val_loss: 0.1411\n",
            "Epoch 35/36\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 32ms/step - accuracy: 0.9956 - loss: 0.0149 - val_accuracy: 0.9767 - val_loss: 0.1359\n",
            "Epoch 36/36\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 32ms/step - accuracy: 0.9956 - loss: 0.0158 - val_accuracy: 0.9761 - val_loss: 0.1342\n"
          ]
        }
      ],
      "source": [
        "#early stop on the epoch 36th\n",
        "\n",
        "# Step 1: Encode labels into integers\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(df['IN code'])  # Convert categorical labels to integers\n",
        "\n",
        "# Step 2: One-hot encode the labels (fix the shape issue)\n",
        "num_classes = 3  # You already know there are 3 unique categories\n",
        "y_one_hot = to_categorical(y_encoded, num_classes=num_classes)\n",
        "\n",
        "# Step 3: Split the data into training and validation sets\n",
        "# Assuming `X` contains your input features (tokenized and padded sequences)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_one_hot, test_size=0.3, random_state=42)\n",
        "X_val,X_test,y_val,y_test = train_test_split(X_val,y_val, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Step 4: Build the LSTM model\n",
        "model = Sequential()\n",
        "\n",
        "# Add embedding layer (make sure vocab_size and embedding_matrix are defined)\n",
        "model.add(Embedding(input_dim=vocab_size,\n",
        "                    output_dim=embedding_dim,\n",
        "                    weights=[embedding_matrix],\n",
        "                    input_length=max_length,\n",
        "                    trainable=False))\n",
        "\n",
        "# Add LSTM layer\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "\n",
        "# Add output layer (number of units must match the number of categories)\n",
        "model.add(Dense(num_classes, activation='softmax'))  # Softmax for multi-class classification\n",
        "\n",
        "# Compile the model\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Define the optimizer with a specific learning rate\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 5: Train the model (fit the model)\n",
        "history = model.fit(X_train, y_train,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    epochs=36,\n",
        "                    batch_size=32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZ6bFOqpFhmg",
        "outputId": "13765e8a-4f6b-453b-b321-094c4fa77b52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8494 - loss: 0.5271\n",
            "Validation Accuracy: 0.8502\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the validation set\n",
        "loss, accuracy = model.evaluate(X_val, y_val)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X_val)\n",
        "predicted_labels = np.argmax(predictions, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Loz5eDE5mqr",
        "outputId": "23991157-4210-4352-c28e-9b8d1cc1fb2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8463 - loss: 0.5499\n",
            "Test Accuracy: 0.8474\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X_test)\n",
        "predicted_labels = np.argmax(predictions, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oChrIZWt9Dv2"
      },
      "source": [
        "**In conclusion, our LSTM model actually perform bettr than the baseline model by outperforming 0.01 accuracy, thus we would select the LSTM model as our optimal model for the classification problem.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DM_sb0Yt9h7w"
      },
      "source": [
        "# Some post thoughts and experimental playgound things"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPUkuSxgMycI"
      },
      "source": [
        "From this point, we had acheived a fairly good accuracy, which fulfilled the goal of this model's initiative. However, the ideal LSTM model should excel the accuracy of our baseline model (95%)by a significant level(let's say 5%). So to speak, can we actually achieve a model that can predict with 100% accuracy? With that said, let's proceed to hyperparameter tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yygW9_iDMBpf"
      },
      "source": [
        "### parameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xx_H2z6HaTvq"
      },
      "outputs": [],
      "source": [
        "x=df_ori['description'].values.tolist()\n",
        "y=df['IN code'].values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQrPF6n-a46J",
        "outputId": "bd94f140-39ab-43f3-b6eb-745783eecc64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13968 13968\n"
          ]
        }
      ],
      "source": [
        "print(len(x),len(y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2DnvgXcbIv_"
      },
      "outputs": [],
      "source": [
        "# define text preprocessing steps\n",
        "lemmatized = []\n",
        "wn = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_corpus(texts):\n",
        "    def remove_stops_digits(tokens):\n",
        "        #Nested function that lowercases, removes stopwords and digits from a list of tokens\n",
        "        tokens_new = [wn.lemmatize(word) for word in tokens ]\n",
        "        return [token.lower() for token in tokens_new if token.lower() not in stopword and not token.isdigit()\n",
        "               and token not in punctuation]\n",
        "    #This return statement below uses the above function to process twitter tokenizer output further.\n",
        "    return [remove_stops_digits(word_tokenize(text)) for text in texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FI4P7DiOb0M8",
        "outputId": "605ca461-24b5-416d-ea39-0dd1924c9397"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13968 13968\n",
            "[['精品黄金梨又甜又脆，口感非常的好，货好的很'], ['酥梨', '600g以上', '膜袋'], ['河北皇冠梨赵县梨新鲜梨子', '品质好', '价格优', '欢迎来聊'], ['万亩梨园有机富硒梨长十郎', '皇冠梨，黄金梨'], ['苏翠一号梨大量上市口感好糖度高'], ['雪花梨，需要的联系'], ['皇冠梨', '450~500g', '纸箱'], ['五九香（八梨）产地直销'], ['河北皇冠梨质优价廉'], ['华山梨大量上市了，有需要的老板赶快下单了']]\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "# get preprossed data and take a look at some sample data\n",
        "X = preprocess_corpus(x)\n",
        "print(len(y), len(X))\n",
        "print(X[:10])\n",
        "print(y[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4ve0D_aF-V_"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df_ori['description'])\n",
        "sequences = tokenizer.texts_to_sequences(df_ori['description'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3DPsZwrB0Yr"
      },
      "outputs": [],
      "source": [
        "max_len = max(len(seq) for seq in sequences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
        "labels = pd.get_dummies(df['IN code']).values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Zc1YOprEYlX"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpApLZeHEftC"
      },
      "outputs": [],
      "source": [
        "train_data = TensorDataset(torch.from_numpy(X_train).long(), torch.from_numpy(y_train).float())\n",
        "val_data = TensorDataset(torch.from_numpy(X_val).long(), torch.from_numpy(y_val).float())\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=50)\n",
        "val_loader = DataLoader(val_data, shuffle=True, batch_size=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_4XOBGQEiMl"
      },
      "source": [
        "Define RNN and LSTM Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bo3rOtEVEitR"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        super(RNN, self).__init__()\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(len(tokenizer.word_index)+1, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        out, hidden = self.rnn(embedded)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc(out[:, -1])\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMuNrrZvEnWX"
      },
      "outputs": [],
      "source": [
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_size, n_layers, drop_prob=0.5):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
        "        lstm_out = self.dropout(lstm_out[:, -1])\n",
        "        out = self.fc(lstm_out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqi_WKCmGcrr"
      },
      "outputs": [],
      "source": [
        "\n",
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ko8K5OjVGhJW",
        "outputId": "dc10162c-5b24-4f58-f499-6648193aef20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Summary for RNN\n",
            "RNN(\n",
            "  (embedding): Embedding(1961, 400)\n",
            "  (rnn): RNN(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=3, bias=True)\n",
            ")\n",
            "\n",
            "\n",
            "Model Summary for LSTM\n",
            "LSTM(\n",
            "  (embedding): Embedding(1961, 400)\n",
            "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=3, bias=True)\n",
            ")\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Initialize models\n",
        "epochs = 20\n",
        "output_size = 3\n",
        "embedding_dim = 400\n",
        "hidden_dim = 256\n",
        "n_layers = 2\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "rnn_model = RNN(output_size, embedding_dim, hidden_dim, n_layers).to(device)\n",
        "lstm_model = LSTM(vocab_size, embedding_dim, hidden_dim, output_size, n_layers).to(device)\n",
        "def print_model_summary(model, model_name):\n",
        "    print(f\"Model Summary for {model_name}\")\n",
        "    print(model)\n",
        "    print(\"\\n\")\n",
        "print_model_summary(rnn_model, \"RNN\")\n",
        "\n",
        "\n",
        "print_model_summary(lstm_model, \"LSTM\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHWJlZjdGrWi"
      },
      "outputs": [],
      "source": [
        "rnn_optimizer = optim.Adam(rnn_model.parameters(), lr=0.001)\n",
        "lstm_optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xefG73u5Gt62"
      },
      "outputs": [],
      "source": [
        "def train_and_validate(model, optimizer, criterion, train_loader, val_loader, epochs, device):\n",
        "    train_losses, val_losses, val_accs = [], [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training mode\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(inputs)\n",
        "            loss = criterion(output, torch.max(labels, 1)[1])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        # Validation mode\n",
        "        model.eval()\n",
        "        total_val_loss, total_acc = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                output = model(inputs)\n",
        "                val_loss = criterion(output, torch.max(labels, 1)[1])\n",
        "                total_val_loss += val_loss.item()\n",
        "                acc = accuracy(output, torch.max(labels, 1)[1])\n",
        "                total_acc += acc.item()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        avg_acc = total_acc / len(val_loader)\n",
        "\n",
        "        train_losses.append(avg_train_loss)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        val_accs.append(avg_acc)\n",
        "\n",
        "        print(f'Epoch: {epoch+1}/{epochs}, Train Loss: {avg_train_loss}, Val Loss: {avg_val_loss}, Val Acc: {avg_acc}')\n",
        "\n",
        "    return train_losses, val_losses, val_accs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zt7LPfniGwRS"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_losses(Model_name, train_losses, val_losses, epochs):\n",
        "    plt.plot(range(1, epochs + 1), train_losses, label='Training Loss')\n",
        "    plt.plot(range(1, epochs + 1), val_losses, label='Validation Loss')\n",
        "    plt.title(f'Training and Validation Losses for {Model_name}')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 865
        },
        "id": "MJ67RBASGzfT",
        "outputId": "419a3196-fb0d-46c1-f4aa-28401c3a1299"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RNN Start Trianing: \n",
            "\n",
            "Epoch: 1/20, Train Loss: 0.2176065880256439, Val Loss: 0.07049702310801617, Val Acc: 0.9846428600805146\n",
            "Epoch: 2/20, Train Loss: 0.3710634886499195, Val Loss: 0.4943156769233091, Val Acc: 0.8147077879735402\n",
            "Epoch: 3/20, Train Loss: 0.19099101423385687, Val Loss: 0.13100855055797314, Val Acc: 0.9685714287417275\n",
            "Epoch: 4/20, Train Loss: 0.09099073977267835, Val Loss: 0.11518524412531406, Val Acc: 0.9785714319774083\n",
            "Epoch: 5/20, Train Loss: 0.26899757522291373, Val Loss: 0.41356084070035387, Val Acc: 0.7742207795381546\n",
            "Epoch: 6/20, Train Loss: 0.1944093646805933, Val Loss: 0.07182267429639719, Val Acc: 0.9863798777971949\n",
            "Epoch: 7/20, Train Loss: 0.07379554878571071, Val Loss: 0.0684183498378843, Val Acc: 0.9877597466111183\n",
            "Epoch: 8/20, Train Loss: 0.13212551400231728, Val Loss: 0.06267638218456081, Val Acc: 0.9882142884390694\n",
            "Epoch: 9/20, Train Loss: 0.07496529445467916, Val Loss: 0.06295428072501506, Val Acc: 0.98678571837289\n",
            "Epoch: 10/20, Train Loss: 0.051194648524480205, Val Loss: 0.5748098513909748, Val Acc: 0.8894480477486338\n",
            "Epoch: 11/20, Train Loss: 0.11090870455414656, Val Loss: 0.06477561494934239, Val Acc: 0.9863798756684575\n",
            "Epoch: 12/20, Train Loss: 0.08718244414714198, Val Loss: 0.08495860052062199, Val Acc: 0.9785227296607835\n",
            "Epoch: 13/20, Train Loss: 0.042558526437330456, Val Loss: 0.06064510318433999, Val Acc: 0.9875000023416111\n",
            "Epoch: 14/20, Train Loss: 0.042154017669547884, Val Loss: 0.05785398302915772, Val Acc: 0.9867370149918965\n",
            "Epoch: 15/20, Train Loss: 0.04445333702044861, Val Loss: 0.050476036965847015, Val Acc: 0.9896428627627236\n",
            "Epoch: 16/20, Train Loss: 0.0483139330359076, Val Loss: 0.046969370696128214, Val Acc: 0.9913798762219292\n",
            "Epoch: 17/20, Train Loss: 0.04468946646998769, Val Loss: 0.04668999427979413, Val Acc: 0.9921428603785378\n",
            "Epoch: 18/20, Train Loss: 0.030755344705539756, Val Loss: 0.055464633935896145, Val Acc: 0.9842857196927071\n",
            "Epoch: 19/20, Train Loss: 0.030668402983012908, Val Loss: 0.03764177370093031, Val Acc: 0.9928571464759963\n",
            "Epoch: 20/20, Train Loss: 0.025352960157241405, Val Loss: 0.04427814087830484, Val Acc: 0.9921428614429065\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "print(\"RNN Start Trianing: \\n\")\n",
        "rnn_train_losses, rnn_val_losses, rnn_val_accs = train_and_validate(rnn_model, rnn_optimizer, criterion, train_loader, val_loader, epochs, device)\n",
        "plot_losses(\"RNN\",rnn_train_losses, rnn_val_losses, epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyAhqHilG4XO"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"LSTM Start Trianing: \\n\")\n",
        "lstm_train_losses, lstm_val_losses, lstm_val_accs = train_and_validate(lstm_model, lstm_optimizer, criterion, train_loader, val_loader, epochs, device)\n",
        "plot_losses(\"LSTM\",lstm_train_losses, lstm_val_losses, epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhsHjWTS0_XD"
      },
      "source": [
        "##CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWTvqGayBCQv"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Dense, Embedding,GlobalMaxPooling1D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "from string import punctuation\n",
        "from os import listdir\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXz_uMSmPK8f",
        "outputId": "9b90b19b-870b-441b-e788-dffc9ae861ff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "13148                [丰县, 大沙河, 红富士, 苹果, 🍎, 上, 元]\n",
              "4705                           [巧儿, 黄金梨, 果乐, 农庄]\n",
              "7806                      [云南, 果然, 小, 杨, 香蕉, 代办]\n",
              "7385                                 [便宜, 香蕉, 找]\n",
              "7029                                    [甜瓜, 香蕉]\n",
              "                          ...                   \n",
              "2464               [晚秋, 黄梨光果, 大量, 上市, 需, 老板, 联系]\n",
              "6621                      [包, 质量, 香蕉, 问题, 全部, 吃]\n",
              "12569      [苹果, 大量, 上市, 客商, 愿意, 前, 采购, 诚信, 保质保量]\n",
              "670                      [贡梨,  , 纸袋, 山西, 高原, 水果]\n",
              "10885    [陕西, 咸阳, 永寿, 红富士, 口感, 好现, 摘, 现卖, 需, 联系]\n",
              "Name: description_token, Length: 11174, dtype: object"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9ttlfPFVXFX"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "\n",
        "vocab_size = 5000\n",
        "# Apply one_hot to each string within the Series\n",
        "X_train = X_train.apply(lambda x: [one_hot(d, vocab_size, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~') for d in x])\n",
        "X_test = X_test.apply(lambda x: [one_hot(d, vocab_size, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~') for d in x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GR5FdmHKV022"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "vocab_size = 5000\n",
        "max_length = 100\n",
        "\n",
        "# Flatten the list of lists after one-hot encoding\n",
        "X_train = X_train.apply(lambda x: [item for sublist in x for item in sublist])\n",
        "X_test = X_test.apply(lambda x: [item for sublist in x for item in sublist])\n",
        "\n",
        "# Now pad the sequences\n",
        "X_train = pad_sequences(X_train, maxlen=max_length, padding='post')\n",
        "X_test = pad_sequences(X_test, maxlen=max_length, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbD74aE7V7hM"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, 8, input_length=max_length),\n",
        "   Conv1D(128, 5, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "  Dense(10, activation='relu'),\n",
        "  Dense(1, activation='sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "od79texDWaz3"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUOw5-v2Wd6_",
        "outputId": "15ef3b57-2643-4489-d5b1-d5fc0ed4d456"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "350/350 [==============================] - 7s 16ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
            "Epoch 2/20\n",
            "350/350 [==============================] - 6s 16ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
            "Epoch 3/20\n",
            "350/350 [==============================] - 5s 15ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
            "Epoch 4/20\n",
            "350/350 [==============================] - 5s 15ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
            "Epoch 5/20\n",
            "350/350 [==============================] - 8s 22ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
            "Epoch 6/20\n",
            "350/350 [==============================] - 6s 18ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
            "Epoch 7/20\n",
            "350/350 [==============================] - 5s 15ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
            "Epoch 8/20\n",
            "350/350 [==============================] - 6s 17ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
            "Epoch 9/20\n",
            "350/350 [==============================] - 3s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
            "Epoch 10/20\n",
            "350/350 [==============================] - 3s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
            "Epoch 11/20\n",
            "350/350 [==============================] - 3s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
            "Epoch 12/20\n",
            "350/350 [==============================] - 5s 13ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
            "Epoch 13/20\n",
            "350/350 [==============================] - 3s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
            "Epoch 14/20\n",
            "350/350 [==============================] - 3s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
            "Epoch 15/20\n",
            "350/350 [==============================] - 3s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
            "Epoch 16/20\n",
            "350/350 [==============================] - 4s 10ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
            "Epoch 17/20\n",
            "350/350 [==============================] - 4s 12ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
            "Epoch 18/20\n",
            "350/350 [==============================] - 3s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
            "Epoch 19/20\n",
            "350/350 [==============================] - 3s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
            "Epoch 20/20\n",
            "350/350 [==============================] - 3s 8ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3354FTyWhiR",
        "outputId": "9ab4d18d-bbb8-4009-cb1c-af67e7266112"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "350/350 [==============================] - 2s 7ms/step - loss: nan - acc: 0.0000e+00\n",
            "Testing Accuracy is 0.0 \n"
          ]
        }
      ],
      "source": [
        "loss, accuracy = model.evaluate(X_train,y_train)\n",
        "print('Testing Accuracy is {} '.format(accuracy*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4fhwnw_S58g"
      },
      "source": [
        "###old version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWhk569nAy08"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nZFDPMsMdk3",
        "outputId": "bf01d2f2-f2f2-4b3c-911c-27fc43bcfd33"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(11174, 100)"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.array(X_train_vect_avg).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4NgJHXJMn1G",
        "outputId": "93e32f5c-6f1f-4fd0-8253-c4a23297fcda"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(11174,)"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 936
        },
        "id": "skf2chN7M-ga",
        "outputId": "2d10a840-a1c8-4ba6-d198-8d922624eb22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(11174, 100)\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 253, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_7' (type Sequential).\n    \n    Input 0 of layer \"conv1d_7\" is incompatible with the layer: expected min_ndim=3, found ndim=2. Full shape received: (None, 100)\n    \n    Call arguments received by layer 'sequential_7' (type Sequential):\n      • inputs=tf.Tensor(shape=(None, 100), dtype=float32)\n      • training=True\n      • mask=None\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-89-cad0e2048675>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_vect_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m history = model.fit(X_train_vect_avg, y_train, epochs=20, \n\u001b[0m\u001b[1;32m      8\u001b[0m                   validation_data=(X_test_vect_avg, y_test))\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 253, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_7' (type Sequential).\n    \n    Input 0 of layer \"conv1d_7\" is incompatible with the layer: expected min_ndim=3, found ndim=2. Full shape received: (None, 100)\n    \n    Call arguments received by layer 'sequential_7' (type Sequential):\n      • inputs=tf.Tensor(shape=(None, 100), dtype=float32)\n      • training=True\n      • mask=None\n"
          ]
        }
      ],
      "source": [
        "# Reshape X_train_vect_avg into a 2D NumPy array\n",
        "X_train_vect_avg = np.array(X_train_vect_avg)\n",
        "\n",
        "# Check the shape to ensure it's a 2D array\n",
        "print(X_train_vect_avg.shape)\n",
        "\n",
        "history = model.fit(X_train_vect_avg, y_train, epochs=20,\n",
        "                  validation_data=(X_test_vect_avg, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "id": "x6gI0nkcKvu4",
        "outputId": "d7fc8cb6-5c32-48b5-be3f-ac64ae165aca"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Data cardinality is ambiguous:\n  x sizes: 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100\n  y sizes: 11174\nMake sure all arrays contain the same number of samples.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-ab5bd019bcfb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model.fit(X_train_vect_avg, y_train, epochs=20, \n\u001b[0m\u001b[1;32m      2\u001b[0m                   validation_data=(X_test_vect_avg, y_test))\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1958\u001b[0m             )\n\u001b[1;32m   1959\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 1...\n  y sizes: 11174\nMake sure all arrays contain the same number of samples."
          ]
        }
      ],
      "source": [
        "history = model.fit(X_train_vect_avg, y_train, epochs=20,\n",
        "                  validation_data=(X_test_vect_avg, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5rWpj2qK78T"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwevvbkq1wzw"
      },
      "outputs": [],
      "source": [
        "   model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation = 'relu'),#, input_shape = (150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation = 'relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dense(6, activation=tf.nn.softmax)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXqXJTaL1_O8"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', # Or any other optimizer\n",
        "              loss='categorical_crossentropy', # Choose appropriate loss for your problem\n",
        "              metrics=['accuracy']) # Add metrics to track"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-w7Ruld1H5vh"
      },
      "outputs": [],
      "source": [
        "history = model.fit(X_train_vect_avg, y_train, batch_size=96, epochs=15, validation_split = 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhbgC8Ux2P50"
      },
      "outputs": [],
      "source": [
        "y_pred_CNN = history.predict(X_test_vect_avg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MoaRWkB2mnR"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test,y_pred_CNN,labels=df['IN code'].unique()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoQzw-7KMEGN"
      },
      "outputs": [],
      "source": [
        "#instantiate model\n",
        "X_train = X_train.to_numpy()\n",
        "X_test =X_test.to_numpy()\n",
        "y_train =y_train.to_numpy().astype('float32')\n",
        "y_test =y_test.to_numpy().astype('float32')\n",
        "print(X_train.shape,y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMCQcQwWQGLp"
      },
      "outputs": [],
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(80,activation='relu', input_shape=(100,)))\n",
        "model.add(layers.Dense(80,activation='relu'))\n",
        "model.add(layers.Dense(1,activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop',#(lr=0.001)\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sd2kLYh_VoXK"
      },
      "outputs": [],
      "source": [
        "print(X_train.shape,y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAxx0JTi56Pt"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXNjavG4KKnZ"
      },
      "outputs": [],
      "source": [
        "#validation data training\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(80,activation='relu', input_shape=(100,)))\n",
        "model.add(layers.Dense(80,activation='relu'))\n",
        "model.add(layers.Dense(1,activation='sigmoid'))\n",
        "\n",
        "\n",
        "model.compile(optimizer='rmsprop',#(lr=0.001)\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "list1 =[1000,5000,8000]\n",
        "list2 = ['bo','yo','go']\n",
        "list3 = ['b','y','g']\n",
        "\n",
        "for i,d,l, in zip(list1,list2,list3):\n",
        "  history = model.fit(X_train,y_train,epochs=50,batch_size=i,validation_split=0.3)\n",
        "  history_dict = history.history\n",
        "  loss_values = history_dict['loss']\n",
        "  val_loss_values = history_dict['val_loss']\n",
        "\n",
        "  epochs = range(1,len(loss_values)+1)\n",
        "\n",
        "  plt.plot(epochs, loss_values, d, label='Training loss')\n",
        "\n",
        "  plt.plot(epochs,val_loss_values, l,label='Validation loss')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ab8YuYdJN5SR"
      },
      "outputs": [],
      "source": [
        "list1 =[12571,1]\n",
        "list2 = [1,12571]\n",
        "list3 = ['bo','yo']\n",
        "list4 = ['b','y']\n",
        "for v1,v2,v3,v4 in zip(list1,list2,list3,list4):\n",
        "  history = model.fit(X_train,y_train,epochs=v1,batch_size=v2,validation_split=0.3)\n",
        "  history_dict = history.history\n",
        "  loss_values = history_dict['loss']\n",
        "  val_loss_values = history_dict['val_loss']\n",
        "\n",
        "  epochs = range(1,len(loss_values)+1)\n",
        "\n",
        "  plt.plot(epochs, loss_values, v3, label='Training loss')\n",
        "\n",
        "  plt.plot(epochs,val_loss_values, v4,label='Validation loss')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfksMYTzFnTB"
      },
      "source": [
        "test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCoX96cOBITx"
      },
      "outputs": [],
      "source": [
        "history = model.fit(X_train,y_train,epochs=100,batch_size=5000,validation_split=0.3)\n",
        "history_dict = history.history\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1,len(loss_values)+1)\n",
        "\n",
        "plt.plot(epochs, loss_values, 'ro', label='Training loss')\n",
        "\n",
        "plt.plot(epochs,val_loss_values, 'r',label='Validation loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHG2sZQkONNo"
      },
      "outputs": [],
      "source": [
        "results = model.evaluate(X_test,y_test)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgVG75g-CT3b"
      },
      "outputs": [],
      "source": [
        "history = model.fit(X_train,y_train,epochs=50,batch_size=5000)\n",
        "results = model.evaluate(X_test,y_test)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJh0HnxrkjM2"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_dict = history.history\n",
        "loss_values = history_dict['loss'][:]\n",
        "val_loss_values = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1,len(loss_values)+1)\n",
        "\n",
        "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
        "#plt.plot(y == min(val_loss_values), 'b',label='Validation loss')\n",
        "plt.plot(epochs,val_loss_values, 'b',label='Validation loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOrmygOkkBac"
      },
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8pCoxJTkEPf"
      },
      "outputs": [],
      "source": [
        "y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8K_eYKA3APz"
      },
      "outputs": [],
      "source": [
        "results = model.evaluate(X_test,y_test)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq-muONtfpB-"
      },
      "source": [
        "save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkaxVVqm4u2Z"
      },
      "outputs": [],
      "source": [
        "#save model\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPxb7BsJ5Wqp"
      },
      "outputs": [],
      "source": [
        "model.save('/content/drive/My Drive/dense_layer')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGVuOO9t5d28"
      },
      "outputs": [],
      "source": [
        " keras.models.load_model('/content/drive/My Drive/dense_layer')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kv_d-DUu49hV"
      },
      "outputs": [],
      "source": [
        "# serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"model.h5\")\n",
        "print(\"Saved model to disk\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "d9_Nm2Rk1w-d",
        "e4fhwnw_S58g"
      ],
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}